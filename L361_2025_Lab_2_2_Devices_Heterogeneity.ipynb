{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b1u66ZKmSCo"
   },
   "source": [
    "# 0. Marking and Guidelines\n",
    "---\n",
    "***IMPORTANT***\n",
    "\n",
    "> The **attendance** and **active participation** in the **lab sessions** is **strongly recommended** and will be considered for grading.\n",
    ">\n",
    "> Save a copy of this notebook into your Drive before you start\n",
    "> \n",
    "> Please attempt all the **questions** marked for your **group** (Part II ✅ | Part III/MPhil ✅).\n",
    "> \n",
    "> Please, provide your answers in a **new cell below the question cell**. You can make as many new cells as you need.\n",
    "\n",
    "Please submit a `.zip` file, containing both parts, consisting of:\n",
    "1. A text file with a **publicly** visible link to your notebooks in GitHub.\n",
    "2. A **downloaded copy** (`.ipynb`) of your notebooks or your zipped cloned GitHub repo. You may treat these as a report---we will not be re-executing the code you used to produce the answers unless required.\n",
    "\n",
    "\n",
    "Feel free to attempt more in case you find yourself enjoying the material!\n",
    "If you have any questions, please ask them to the teaching assistants.\n",
    "Are you interested in knowing more about federated learning and related topics? Reach out to the teaching assistants for additional resources and ask more about the current research projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUKMZp23mSCr"
   },
   "source": [
    "## 1. Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2DNy1LXmSCs"
   },
   "source": [
    "Welcome to the third lab session in our FL course.\n",
    "We now know how to “federate” a centralised ML model and have learned some tools to deal with data heterogeneity.\n",
    "\n",
    "During this session, we will try to put our hands on system heterogeneity in simulating FL. However, assuming you want to produce an FL system that will be deployed in the real world, taking into account data heterogeneity is often not enough.\n",
    "\n",
    "Client hardware may be correlated with the underlying data---e.g. a smartphone's camera impacts image characteristics---or with which data is used. For example, clients with less reliable internet connections from specific regions of the world are more likely to drop out, making their region underrepresented in training. Thus, the limits placed on computation time or the assumptions made in applying a synchronous or asynchronous algorithm affect model performance, fairness, or scalability.\n",
    "\n",
    "Developers may want to simulate their FL pipelines in a controlled environment in which natural system characteristics are modelled in order to be able to understand such trade-offs.\n",
    "\n",
    "What should we take into account, then?\n",
    "Clients' availability, training time, communication bandwidth, and other factors sometimes impact natural FL systems unexpectedly.\n",
    "Developers usually set up constraints for selecting clients to account for most of these factors, but in the worst case, these may result in not completing even a single round.\n",
    "On the other hand, relaxing some constraints produces long round completion times, and eventually, the FL model cannot reach convergence in a reasonable interval.\n",
    "\n",
    "Before we begin modelling such concerns, we shall investigate data regarding real-world system characteristics from the two following papers:\n",
    "\n",
    "1. [Papaya: Practical, Private, and Scalable Federated Learning](https://arxiv.org/abs/2111.04877)\n",
    "2. [Towards Federated Learning at Scale: System Design](https://arxiv.org/abs/1902.01046)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl5cv19OkMow"
   },
   "source": [
    "## 2. Behavioural patterns of real-world FL systems\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2YlPgJumSCs"
   },
   "source": [
    "The following plot was extracted from the Papaya paper (1) mentioned above.\n",
    "The takeaway is that in natural settings, devices are very heterogeneous, and the same model could take different amounts of time to train on various clients. Since the distribution is neither Gaussian nor uniform, accounting for such behaviour is non-trivial. As a result, developers often have to make imperfect choices when selecting between synchronous and asynchronous FL algorithms and their parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AXtCGbgmSCt"
   },
   "source": [
    "![client_execution_time](../assets/client_execution_time_histo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zE3-F_r_kMow"
   },
   "source": [
    "In the case of synchronous training, they may select clients based on hardware, set time limits for round completion, and incorporate partially trained client models that have not reached the necessary step/epoch count.\n",
    "\n",
    "Besides tackling the previously discussed data heterogeneity, FedProx was designed to better incorporate partially trained models from stragglers by limiting their impact upon the aggregation. The dual purpose of FedProx illustrates the interdependence of systems and data heterogeneity.\n",
    "\n",
    "In the case of asynchronous training, all updates **could** be considered; however, model staleness and a significant bias towards faster clients become major issues. For example, clients who return updates based on an older model may have had more data to train on or simply slower internet. If they have more data, they might provide more **statistical utility** than fast clients and thus improve final accuracy. Alternatively, they might have valuable data from underrepresented or remote regions if they have slower internet. If you are curious and want more insights about asynchronous training in FL, we point to the most prominent example: [FedBuf](https://arxiv.org/abs/2106.06639).\n",
    "\n",
    "In an extreme case of hardware heterogeneity, clients with high-end hardware may guide the model to the detriment of those with better data or those belonging to relevant subgroups. Due to such difficulties, asynchronous FL has been historically more difficult to implement and less preponderantly used than one may expect, given its potential benefits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv2XIa5qkMow"
   },
   "source": [
    "---\n",
    "\n",
    "**Question 7 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. **No code** is needed)\n",
    "\n",
    "1. In the context of a complete synchronous FL training period composed of multiple rounds, when do you think it would be appropriate to oversample fast/slow clients? Should it be more appropriate for the early or late training phases? Think of which characteristics are essential for convergence in the early versus late rounds.\n",
    "2. Can you think of a heuristic for modifying the time threshold across rounds that considers your previous answer?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4VcVSTNkMow"
   },
   "source": [
    "---\n",
    "\n",
    "**Question 8 (Part III/MPhil ✅):**\n",
    "\n",
    "(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. **No code** is needed)\n",
    "\n",
    "1. What parallels can be drawn between asynchronous SGD and asynchronous FL? Are there any methods which may help with both?\n",
    "2. How do you think a system using a buffer to accumulate gradient updates received asynchronously, such as [FedBuf](https://arxiv.org/pdf/2106.06639.pdf), behaves?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDgqtaRHkMox"
   },
   "source": [
    "### 2.1 Cyclical patterns\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dycOBwNSmSCt"
   },
   "source": [
    "The plot below is taken from paper (2) mentioned above and encapsulates the reliability of client training. In the context of mobile devices, we can observe a cyclic daily trend for each category plotted while the delta between the categories is partly conserved. Thus, according to the number of clients we expect to finish training, we can adapt our training parameters, such as a completion-time threshold.\n",
    "\n",
    "If the federation included a more diverse set of devices operating in different domains or across a wider geographic area, the number of completing, aborted or dropped-out devices may not show the same synchronised patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsqU3vK1mSCt"
   },
   "source": [
    "![aborted_completed_dropped_zoom](../assets/device_per_round_timeseries.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3vWOvDMmSCu"
   },
   "source": [
    "These plots from paper (2) show a more clear cyclic trend in the participation rate, completion rate and network utilisation of clients depending on the day-night cycle. It is fair to say that these measurements were taken from devices in the same time zone and reflect common smartphone usage patterns related to humans' sleep and activity cycles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCIR2EH5mSCu"
   },
   "source": [
    "![connected_round_completion](../assets/connected_and_rct_timeseries.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lh7CtpwtmSCv"
   },
   "source": [
    "![network_traffic](../assets/server_network_traffic_timeseries.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSzyvZn0kMox"
   },
   "source": [
    "---\n",
    "\n",
    "**Question 9 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. **No code** is needed)\n",
    "\n",
    "1. What would happen to device availability if we scaled the federated network globally?\n",
    "2. How would the time of day interact with the client data and system characteristics in this new global federated network?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDH3cqcKkMox"
   },
   "source": [
    "---\n",
    "\n",
    "**Question 10 (Part III/MPhil ✅):**\n",
    "\n",
    "(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. **No code** is needed)\n",
    "\n",
    "Beyond shifts in the devices used at a specific time, on a long-enough timescale, FL also suffers from **non-cyclical** changes in the underlying hardware and data distributions which can lead to them getting desynced. For example, users may transfer their old data to a new device with more powerful sensors and thus create two “tasks'' instead of one. Furthermore, they can delete portions of the older data over time and shift the balance between the two tasks.\n",
    "\n",
    "1. For FL systems with more powerful and long-lasting clients, one potential method for handling such data and hardware shift is maintaining a persistent local model trained concurrently with the federated model---e.g. via [mutual knowledge distillation](https://arxiv.org/abs/1706.00384). This model may regularise the federated one and encodes valuable information about the data distribution accumulated at different points in time. Considering that such a model never leaves the client and thus does not have significant privacy concerns, what techniques would you apply to make it as **informative to the federated model as possible?**\n",
    "\n",
    "2. Assuming that the federated network has changed too much for an old federated model to perform well, how would you use the old model to bootstrap a new one?\n",
    "\n",
    "For both components of the question, you may draw inspiration from techniques used in [continual learning](https://arxiv.org/pdf/1909.08383.pdf), [mutual learning](https://arxiv.org/abs/1706.00384) or any other relevant field of ML.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnTGe6HTkMox"
   },
   "source": [
    "## 3. Modelling system heterogeneity\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYoFUSwGmSCv"
   },
   "source": [
    "### Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhBh8fstmSCv"
   },
   "source": [
    "The following cell will download the relevant python packages using `pip`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45960,
     "status": "ok",
     "timestamp": 1676716681758,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "NuHXppIqmSCv",
    "outputId": "76b62fee-1187-4238-8d8d-ea0fc623c844"
   },
   "outputs": [],
   "source": [
    "# `pip` could produce some errors. Do not worry about them.\n",
    "# The execution has been verified; it's working anyway.\n",
    "! pip install --quiet --upgrade \"pip\"\n",
    "! pip install --quiet matplotlib tqdm seaborn\n",
    "! pip install git+https://github.com/Iacob-Alexandru-Andrei/flower.git@teaching \\\n",
    "    torch torchvision ray==\"2.6.3\"\n",
    "# The following is just needed to show the folder tree\n",
    "! apt-get install -qq tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB_MjsSBmSCw"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import random\n",
    "import pickle\n",
    "from collections.abc import Callable\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from logging import INFO\n",
    "\n",
    "\n",
    "import flwr as fl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from flwr.common import (\n",
    "    log,\n",
    ")\n",
    "from flwr.common.typing import NDArrays, Scalar\n",
    "from flwr.server import ServerConfig, History\n",
    "from flwr.server.server_returns_parameters import ReturnParametersServer as Server\n",
    "from flwr.server.strategy import Strategy\n",
    "from flwr.client import Client\n",
    "from torch.nn import Module\n",
    "from enum import IntEnum\n",
    "\n",
    "from common.client_utils import (\n",
    "    IntentionalDropoutError,\n",
    "    save_history,\n",
    "    ModelSizeNotFoundError,\n",
    ")\n",
    "from common.client import FlowerClient\n",
    "from common.client_utils import get_device\n",
    "\n",
    "\n",
    "# Add new seeds here for easy autocomplete\n",
    "class Seeds(IntEnum):\n",
    "    \"\"\"Seeds for reproducibility.\"\"\"\n",
    "\n",
    "    DEFAULT = 1337\n",
    "\n",
    "\n",
    "np.random.seed(Seeds.DEFAULT)\n",
    "random.seed(Seeds.DEFAULT)\n",
    "torch.manual_seed(Seeds.DEFAULT)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "PathType = Path | str | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDJzls21mSCw"
   },
   "source": [
    "In the following cell, we will download the relevant file we need for this session.\n",
    "Feel free to look into this material if you want.\n",
    "There is nothing new compared to Lab 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2LfZv9BmSCw"
   },
   "source": [
    "### Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = Path.cwd()\n",
    "dataset_dir: Path = home_dir / \"femnist\"\n",
    "data_dir: Path = dataset_dir / \"data\"\n",
    "centralized_partition: Path = dataset_dir / \"client_data_mappings\" / \"centralized\"\n",
    "centralized_mapping: Path = dataset_dir / \"client_data_mappings\" / \"centralized\" / \"0\"\n",
    "federated_partition: Path = dataset_dir / \"client_data_mappings\" / \"fed_natural\"\n",
    "# NEW\n",
    "devices_info_dir: Path = home_dir / \"device_info\"\n",
    "statistical_utility: Path = home_dir / \"statistical_utility.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_seeded_simulation(\n",
    "    client_fn: Callable[[str], Client],\n",
    "    num_clients: int,\n",
    "    config: ServerConfig,\n",
    "    strategy: Strategy,\n",
    "    name: str,\n",
    "    seed: int = Seeds.DEFAULT,\n",
    "    server: Server | None = None,\n",
    "    iteration: int = 0,\n",
    ") -> tuple[list[tuple[int, NDArrays]], History]:\n",
    "    \"\"\"Wrap simulation to always seed client selection.\"\"\"\n",
    "    np.random.seed(seed ^ iteration)\n",
    "    torch.manual_seed(seed ^ iteration)\n",
    "    random.seed(seed ^ iteration)\n",
    "    parameter_list, hist = fl.simulation.start_simulation_no_ray(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=num_clients,\n",
    "        server=server,\n",
    "        client_resources={},\n",
    "        config=config,\n",
    "        strategy=strategy,\n",
    "    )\n",
    "    save_history(home_dir, hist, name)\n",
    "    return parameter_list, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145828,
     "status": "ok",
     "timestamp": 1676716833352,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "Yuv_baiikMoy",
    "outputId": "6905f7d3-b031-4ca8-e6e6-b64d88996ac2"
   },
   "outputs": [],
   "source": [
    "# Decompress dataset\n",
    "if not dataset_dir.exists():\n",
    "    with tarfile.open(home_dir / \"femnist.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall(path=home_dir)\n",
    "    log(INFO, \"Dataset extracted in %s\", dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esYTgkRSmSCx"
   },
   "source": [
    "### Extract system traces\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmoZgo6mSCx"
   },
   "source": [
    "To model system heterogeneity, we will use a collection of actual device traces and capabilities extracted by another FL framework: [FedScale](https://github.com/SymbioticLab/FedScale). Unfortunately, these traces are not directly coupled to the FEMNIST dataset we have been using. As such, they cannot represent an intrinsic relation between data and system characteristics; we have to devise a mapping scheme between them. Generally, the pervasive lack of datasets synced to hardware characteristics in Federated Learning makes simulations unreliable as a source of guidance for production scenarios.\n",
    "\n",
    "A complete description of this data is available in [FedScale's](https://arxiv.org/abs/2105.11367) [paper](https://arxiv.org/abs/2105.11367) Sec. 3.2. Also, inside the folder you will download, there is a `README.md` file containing the minimal description of the files inside the folder.\n",
    "\n",
    "The first type of trace represents device communication and computation as floating point numbers. They can calculate a theoretical computation speed for each client for the given model, batch size and the number of batches.\n",
    "\n",
    "```\n",
    "{\n",
    "  'computation': FP32,\n",
    "  'communication': FP32,\n",
    "}\n",
    "```\n",
    "\n",
    "The inherent assumption of the formula we shall use is that the relative ordering of devices does not change according to the task. Even if performance changes between tasks by orders of magnitude---and recent papers indicate that it can---as long as the ordering is conserved, we can adjust the thresholds of our experiments and obtain broadly consistent results. Explicitly, we assume that if one device performed inference faster than another in the original benchmark, it should do so for any model and data combination. However, this is not guaranteed to hold in a modern hardware landscape, and we would ideally need regularly updated system traces for every kind of data---e.g. image, text.\n",
    "\n",
    "A second type of trace contains data on client activity and is used to determine when it can be selected for training.\n",
    "\n",
    "```\n",
    "{\n",
    "  'duration': INT,\n",
    "  'inactive': [INT],\n",
    "  'finish_time': INT,\n",
    "  'active': [INT],\n",
    "  'model': STRING\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1725,
     "status": "ok",
     "timestamp": 1676716839326,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "n82bqNhCmSCx",
    "outputId": "dbafa1fa-f107-438f-be71-be566b804dc1"
   },
   "outputs": [],
   "source": [
    "# Decompress dataset\n",
    "if not devices_info_dir.exists():\n",
    "    with tarfile.open(home_dir / \"device_info.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall(path=home_dir)\n",
    "    log(INFO, \"Devices' info extracted in %s\", devices_info_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znAQLwX7mSCx"
   },
   "source": [
    "We will implement three functions which will help us in the following discussion.\n",
    "\n",
    "1. The first `get_devices_info` can be used to put the info we just downloaded into `pandas` `DataFrame` structures.\n",
    "2. The second `is_active` can compute whether a client is active, given its device traces and the virtual clock.\n",
    "3. Finally, the third function `get_client_completion_time` is just computing the estimates for communication time and computation time of the client with the assumptions outlined above while incorporating a correction factor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "an8QeXjxmSCx"
   },
   "outputs": [],
   "source": [
    "def get_devices_info(\n",
    "    root_dir: Path,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Retrieve two pandas data frames with traces and capabilities of clients' devices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        root_dir (Path): path to the folder containing such traces.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: couple of dataframes containing the\n",
    "            requested info\n",
    "    \"\"\"\n",
    "    cbt_path = root_dir / \"client_behave_trace\"\n",
    "    cdc_path = root_dir / \"client_device_capacity\"\n",
    "    with open(cbt_path, \"rb\") as f:\n",
    "        client_behave_trace = pd.DataFrame(pickle.load(f)).transpose()\n",
    "    with open(cdc_path, \"rb\") as f:\n",
    "        client_device_capacity = pd.DataFrame(pickle.load(f)).transpose()\n",
    "    return client_behave_trace, client_device_capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkrXB2AKkMoz"
   },
   "source": [
    "Determining whether a client is active requires considering the cyclical behaviour described above. Specifically, our clock is defined by the `finish_time` integer inside the traces' data frame; the next cycle begins once the finish time is exceeded. We normalise all times by this `finish_time` value using the modulo operator. This normalised time allows us to maintain a virtual clock for client synchronisation in FL.\n",
    "\n",
    "Given the normalised time, each client is a sequence of active and inactive periods that wraps back at the end. The first inactive period is always chronologically after the first active period. However, not all the traces start at time 0. For consistency, we have considered the period between 0 and the beginning of the first active period to be **inactive**. This convention fits the idea that a client must join the federation before they can become active within it.\n",
    "\n",
    "A given client is considered active at the **start of a round** if its most recent active period before the current time is later than its most recent inactive period.\n",
    "\n",
    "> **IMPORTANT**: Activity at the start of the round **does not imply activity throughout the round**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kras9RyHmSCx"
   },
   "outputs": [],
   "source": [
    "def is_active(\n",
    "    single_client_traces: dict[str, Any],\n",
    "    current_clock_time: int,\n",
    ") -> bool:\n",
    "    \"\"\"Return a boolean describing whether the client is active or not.\n",
    "\n",
    "    It returns True when `single_client_traces` are not given.\n",
    "    The current (virtual) clock time must pass as a parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        single_client_traces (dict[str, Any]): dict describing client device traces.\n",
    "        current_clock_time (int): parameter that describes current (virtual) clock time.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        bool: True is the client is active, False elsewhere.\n",
    "    \"\"\"\n",
    "    # If no traces are given, return True\n",
    "    if single_client_traces is None:\n",
    "        return True\n",
    "    # Get the normalized time when the `current_clock_time` is\n",
    "    # greater than `single_client_traces['finish_time']`, nothing\n",
    "    # happens if `current_clock_time` < `single_client_traces['finish_time']`\n",
    "    normalized_time = current_clock_time % single_client_traces[\"finish_time\"]\n",
    "    # Get the highest single_client_traces['active'] occurrence\n",
    "    # that is lower than `normalized_time`\n",
    "    single_client_traces[\"active\"].sort()\n",
    "    active_time = -1\n",
    "    for t in single_client_traces[\"active\"]:\n",
    "        active_time = t if t <= normalized_time else active_time\n",
    "    # print(f\"Highest active time: {active_time}\")\n",
    "    # Get the highest single_client_traces['inactive'] occurrence\n",
    "    # that is lower than `normalized_time`\n",
    "    single_client_traces[\"inactive\"].sort()\n",
    "    inactive_time = -1\n",
    "    for t in single_client_traces[\"inactive\"]:\n",
    "        inactive_time = t if t <= normalized_time else inactive_time\n",
    "    # print(f\"Highest inactive time: {inactive_time}\")\n",
    "\n",
    "    return active_time > inactive_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3BPsUrtkMoz"
   },
   "source": [
    "Estimating a given client's time to finish the workload is more straightforward, given our assumption of universal computation performance **across** tasks. However, one relevant detail is the need to double-count communication costs in the equation as the model needs to be transmitted both forwards and backwards. Please do not concern yourself with the normalisation of compute nor the scaling `augmentation_factor` unless you consider it necessary for a future experiment.\n",
    "\n",
    "One variable which will require tuning is `model_size_scale_factor`, as it allows us to model how the computation-communication trade-off changes when we increase or decrease the size of an actual ML model. For the rest of this lab the size of the model shall be assumed to be in MB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71ltPemfmSCy"
   },
   "outputs": [],
   "source": [
    "def get_client_completion_time(\n",
    "    single_client_device_capacity: dict[str, Any],\n",
    "    batch_size: int,\n",
    "    n_batches: int,\n",
    "    model_size: float,\n",
    "    augmentation_factor: float = 3.0,\n",
    "    model_size_scale_factor: float = 1.0,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Compute the computation and communication latency of the client.\n",
    "\n",
    "    These values are computed as follows:\n",
    "    - Computation latency: `single_client_device_capacity['computation']` is the\n",
    "        inference latency of models (ms/sample). We compute the computation latency as\n",
    "        the inference latency times the number of samples processed. As reported in\n",
    "        many papers, backward-pass takes around 2x the latency, so we multiply it by 3x.\n",
    "    - Communication latency: `single_client_device_capacity['communication']` represents\n",
    "        the bandwidth of the device (kB/s). We then compute the communication latency as\n",
    "        the ratio between twice the size of the model and the bandwidth of the device.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        single_client_device_capacity (dict[str, Any]): dictionary containing info about\n",
    "            device capabilities.\n",
    "        batch_size (int): batch size used during local client training.\n",
    "        n_batches (int): number of batches trained by the client.\n",
    "        model_size (float): an estimate of the size of the model in MB.\n",
    "        augmentation_factor (float, optional): multiplicative augmentation factor for\n",
    "            the computation latency. Defaults to 3.0.\n",
    "        model_size_scale_factor (float, optional): multiplicative augmentation factor\n",
    "            for the communication latency. Defaults to 1.0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Dict[str, float]: dictionary containing estimates for time spent by the client\n",
    "            in computation and communication.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"computation\": (\n",
    "            augmentation_factor\n",
    "            * batch_size\n",
    "            * n_batches\n",
    "            * float(single_client_device_capacity[\"computation\"])\n",
    "            / 1000.0\n",
    "        ),\n",
    "        \"communication\": (\n",
    "            2\n",
    "            * model_size_scale_factor\n",
    "            * model_size\n",
    "            * 1000\n",
    "            / float(single_client_device_capacity[\"communication\"])\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAaSFZYHkMoz"
   },
   "outputs": [],
   "source": [
    "client_behave_trace, client_device_capacity = get_devices_info(devices_info_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUCWOwKVmSCy"
   },
   "source": [
    "### Analyse what's inside these traces\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ-mkQH9kMo0"
   },
   "source": [
    "In the following few cells, we will analyse the traces regarding the distributions of the overall population.\n",
    "The quantities we are primarily interested in are the `duration` and `finish_time` from `client_behave_trace`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQAbYDUakMo0"
   },
   "source": [
    "If we plot the duration of training for clients, the distribution should resemble the first plot we saw in this notebook. This `duration` value is the sum of the periods of time in which each device is active. Although, we will interpret it as the client execution time that can be computed as the sum of the outputs of `get_client_completion_time`. FedScale's authors used the `duration` value as the initial estimate for the client completion time. We will use it in this section as the client completion time for “standard” values of `batch_size` and `num_batches`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "executionInfo": {
     "elapsed": 1309,
     "status": "ok",
     "timestamp": 1676716868037,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "12ms9fFUmSCy",
    "outputId": "93a42e37-02e9-45b0-d229-a30308b583d8"
   },
   "outputs": [],
   "source": [
    "client_behave_trace.duration.hist(bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpkTnZwVkMo0"
   },
   "source": [
    "The subsequent distribution should show us how different the activity cycles recorded in the traces are. Due to the considerable variation in finish times, it was necessary to use the `finish_time` as a normalisation factor above to simulate devices operating in the same period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "executionInfo": {
     "elapsed": 1069,
     "status": "ok",
     "timestamp": 1676716869101,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "ognZeZMYkMo0",
    "outputId": "0f03c898-82ad-4c93-f74a-3d85d269674a"
   },
   "outputs": [],
   "source": [
    "client_behave_trace.finish_time.hist(bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3DbuYinwQXf"
   },
   "source": [
    "One of the choices we had to make when modelling client activity concerned handling the period between 0 and when a client first becomes active; we can now inspect the motivating factors behind treating that period as inactive.\n",
    "\n",
    "Our choice was theoretically justifiable and practical given the start-times distribution we see below. The peak around 0 is natural, given that this was when the recording started. However, it may create difficulties during FL simulations as it would allow many clients to be available immediately for the first few rounds. Additionally, as all clients would be right at the beginning of their active period for the first round, it would create a scenario with minimal drop-out rates.\n",
    "\n",
    "> By treating everyone who starts later than 0 as inactive at the start, we lower the potential bias towards the early rounds that a simulation may have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 1751,
     "status": "ok",
     "timestamp": 1676716870850,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "ZetFsdEml6KZ",
    "outputId": "1f87f22c-de62-42b1-c239-dff6c386eb38"
   },
   "outputs": [],
   "source": [
    "# New column containing the start time of\n",
    "# each client defined as the beginning\n",
    "# of their first active period\n",
    "client_behave_trace[\"first_active\"] = client_behave_trace[\"active\"].map(lambda x: x[0])\n",
    "client_behave_trace.first_active.hist(bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHYwXHoUkMo1"
   },
   "source": [
    "---\n",
    "\n",
    "**Question 11 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n",
    "\n",
    "1. Randomly sample 100 behavioural traces from `client_behave_trace`. Sample without replacement. Plot the total number of active and inactive clients over time for 600000 seconds using steps of 10 seconds.\n",
    "2. Sort `client_behave_trace` by `finish_time` in **ascending** order. Call it `ft_sorted_client_behave`. Repeat point (1) using `ft_sorted_client_behave` instead of `client_behave_trace`. Replace the random sampling with a sampling of the first 100 traces.\n",
    "3. Sort `client_behave_trace` by `duration` in **descending** order. Call it `dur_sorted_client_behave`. Repeat point (1) using `dur_sorted_client_behave` instead of `client_behave_trace`. Replace the random sampling with a sampling of the first 100 traces.\n",
    "4. Compare the three plots. What do you observe? What are the implications of performing FL with such populations?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9Fb96C19e2a"
   },
   "source": [
    "---\n",
    "\n",
    "**Question 12 (Part III/MPhil ✅):**\n",
    "\n",
    "(This is meant to be a conceptual question. You should provide a written answer to this. **No more than 3 sentences each**. **No code** is needed)\n",
    "\n",
    "1. In light of your consideration about question (5) above, what do you think could happen when reverting the ordering in point (2) and point (3)? Focus on the plots.\n",
    "2. What are the implications of the FL training with such populations? Refer to populations with reverse ordering in point (2) and point (3).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKSr0evy97X4"
   },
   "source": [
    "### Analyse what's inside device capabilities\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssx7BRGG-LR0"
   },
   "source": [
    "In the next few cells, we will analyse the content of `client_device_trace`.\n",
    "Primarily, we will focus on the distribution of `computation` and `communication` on the clients and the afferent trade-off between the two, given a specific model size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBVxtIFokMo0"
   },
   "source": [
    "As we could have imagined, the devices' computational capabilities are highly heterogeneous. For example, despite the consistent peak at around 25 ms/sample, there is a relevant part of the population of devices with values 6 to 8 times higher. Suppose such heterogeneity in computation was related to factors influencing the data distribution. In that case, it could induce significant bias in the model training procedure, which would have to be considered by the developer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1676716870851,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "O_02l1kakMo0",
    "outputId": "79978ac3-7f7f-49d3-984d-617378947796"
   },
   "outputs": [],
   "source": [
    "client_device_capacity.computation.hist(bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOuygWirkMo0"
   },
   "source": [
    "The communication capabilities are also very heterogeneous, as expected. Despite the distribution having a more apparent peak with a shorter tail, some devices have communication capabilities orders of magnitude higher than the peak of the distribution.\n",
    "\n",
    "Geographic variance in network speed is non-uniform and related to the data a given client may contain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 2059,
     "status": "ok",
     "timestamp": 1676716872906,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "L2jyeza-kMo0",
    "outputId": "c3bc3b81-0d20-475b-a603-a593764965f7"
   },
   "outputs": [],
   "source": [
    "client_device_capacity.communication.hist(bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1676716872906,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "MKgeroBhp8bo",
    "outputId": "0ebdc46b-206d-4171-fc3d-095eb1c6bb75"
   },
   "outputs": [],
   "source": [
    "client_device_capacity.communication.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have previously noticed, the traces for the device compute capabilities and the traces for availability are uncorrelated. This is due to a lack of datasets in FL which synchronously cover both.\n",
    "\n",
    "In the following question you will be asked to numerically verify this lack of correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQC-l65_08cW"
   },
   "source": [
    "---\n",
    "\n",
    "**Question 13 (Part II ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n",
    "\n",
    "Looking carefully at the number in the y-axis, some of you may have noticed that the number of samples retrieved from `client_behave_trace` is not the same as that retrieved from `client_device_capacity`. Given this fact, we may wish to investigate if they are drawn from the same population of devices.\n",
    "\n",
    "1. Plot communication and computation mapped one-to-one using the entirety of `client_device_capacity`.\n",
    "\n",
    "- Do you notice any correlation between the two traces as given?\n",
    "\n",
    "2. Fix two different thresholds for computation, and plot the histogram of communication costs from the previous mapping using only traces whose computation falls below the given threshold.\n",
    "\n",
    "- Do these histograms look different from one another?\n",
    "- Do they look different from the overall communication histogram?\n",
    "\n",
    "3. Fix two communication thresholds and plot the histograms of computation costs of traces with communication values falling below the thresholds.\n",
    "\n",
    "- Do these histograms look different from one another?\n",
    "- Do they look different from the overall computation histogram?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6vjl2zFkMo0"
   },
   "source": [
    "The mismatch between the two types of traces is because the authors of FedScale have combined two separate datasets to construct the files. They used [AI benchmark](https://arxiv.org/abs/1910.06663) to obtain the performance capabilities of mobile devices and [MobiPerf](https://www.measurementlab.net/tests/mobiperf/) to gather the availability traces.\n",
    "\n",
    "Similarly to the mismatch between traces and data we have previously discussed, experiments using performance and availability traces collected from separate devices may not generalise to the real world. However, since we do not yet have the large-scale datasets necessary to model FL systems accurately, we will have to accept this inconsistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1676716872907,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "IFVCdY_8kMo1",
    "outputId": "bc3c3190-e429-422e-98ce-0290ac31bbd0"
   },
   "outputs": [],
   "source": [
    "log(\n",
    "    INFO,\n",
    "    \"The number of samples in `client_behave_trace` is %s\",\n",
    "    len(client_behave_trace),\n",
    ")\n",
    "log(\n",
    "    INFO,\n",
    "    \"The number of samples in `client_device_capacity` is %s\",\n",
    "    len(client_device_capacity),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtWt7x2QkMo1"
   },
   "source": [
    "Because the clients do not come from the same population, we need to devise a method for synchronising them.\n",
    "The most straightforward procedure we can devise is to sample `len (client_behave_trace)` data points from `client_device_capacity` uniformly. Many other methods that can extract distributions with particular characteristics are possible, but we will do this for brevity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1676716872907,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "4x5gU32_kMo1",
    "outputId": "3fea272b-1f82-4ca1-ce03-cecac0d0ea99"
   },
   "outputs": [],
   "source": [
    "client_device_capacity = client_device_capacity.sample(\n",
    "    len(client_behave_trace), replace=False, random_state=Seeds.DEFAULT\n",
    ")\n",
    "log(\n",
    "    INFO,\n",
    "    \"The `client_device_capacity` obtained is %s\",\n",
    "    client_device_capacity,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Sff3LPFkMo1"
   },
   "source": [
    "We can plot the distributions from `client_device_capacity` to see if the sampling procedure has succeeded before we apply it in our experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "executionInfo": {
     "elapsed": 1835,
     "status": "ok",
     "timestamp": 1676716874733,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "GDLgodTwp8bp",
    "outputId": "04724a55-3a15-4364-afdc-219303058589"
   },
   "outputs": [],
   "source": [
    "client_device_capacity.computation.hist(bins=\"auto\")\n",
    "plt.show()\n",
    "client_device_capacity.communication.hist(bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oADe4R4sF96"
   },
   "source": [
    "For later experiments, you will have to use a computation threshold to select clients. The following plot is likely the most revealing if you wish to understand better the impact a particular computation threshold will have on the number of clients that may be included in a given round. Pay particular attention to the sharp increase at the start of the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1676716875053,
     "user": {
      "displayName": "Alexandru-Andrei Iacob",
      "userId": "00751686620367198202"
     },
     "user_tz": 0
    },
    "id": "Ono9HL-jsA2l",
    "outputId": "0e153711-ebc0-4d2a-ab3a-183d47ac9ffe"
   },
   "outputs": [],
   "source": [
    "client_device_capacity.computation.hist(bins=\"auto\", cumulative=True, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hal8zjei_fUL"
   },
   "source": [
    "---\n",
    "\n",
    "**Question 14 (Part II ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n",
    "\n",
    "We have just extracted and analysed the device capabilities of our population and obtained some idea about how communication and computation capabilities are distributed. However, the exact trade-off depends on `model_size`, `batch_size` and `n_batches`. Since `batch_size*n_batches` is fixed to equal the local dataset size, `model_size` requires closer attention for our modelling of FL systems to be informative. Specifically, we want both communication and computation to have detectable impacts on your FL experiments and their interaction shifts as you scale a network.\n",
    "\n",
    "Our function `get_client_completion_time` can help us observe the trade-off between computation and communication without running costly FL experiments. For the following question, assume that `batch_size=32`, `n_batches=64` stay fixed.\n",
    "\n",
    "1. Compute the average client completion time for different values of `model_size`. The average is intended to be over the population described by `client_device_capacity`. Choose at least 100 different values of the `model_size` in the interval `[0,2000] MB`.\n",
    "2. Plot your results from (1) as explained here: from (1) you will have the average communication latency (`a`), the average computation latency (`b`), and you can compute their sum (`c`). Then, plot those three curves (`a`, `b`, and `c`) in the same graph VS model size. The intersection point between `a` and `b` must be visible. What does it represent?\n",
    "3. Extract the model size from the intersection point in (2).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41vY3KUBmSC0"
   },
   "source": [
    "## 4. Implementing system traces in the simulation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZxm5U12mSC0"
   },
   "source": [
    "To implement our system traces into the Flower simulator to model a real FL federation, we need to extend the abstraction of `FlowerRayClient`. The new version of the abstraction shall include the information we have extracted from the traces. Achieving this will require creating a new class `FlowerRayClientTraces`, which inherits from `FlowerRayClient`.\n",
    "\n",
    "The new class will have minimal new attributes and functions needed to model the actual system accurately. We will simulate clients dropping out of the federation by throwing exceptions and allowing Flower to accept failures.\n",
    "\n",
    "The `get_flower_client_generator` function will be modified to return a generator of `FlowerRayClientTraces` instead of `FlowerRayClient`. In addition, we will pass the traces that must be coupled to the clients as parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7w3rIsemSC0"
   },
   "source": [
    "### Implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaqmSf5Up8bp"
   },
   "source": [
    "We will also use previously unrevealed Flower features, such as client properties and criteria, so pay attention to those details. In short, clients can return properties to a criterion after they have been instantiated. The criterion then decides if the client should be included in the next federated round based on those properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAw468w6kMo2"
   },
   "outputs": [],
   "source": [
    "class FlowerClientTraces(FlowerClient):\n",
    "    \"\"\"Extend the FlowerClient class to include traces and device capabilities.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cid: int,\n",
    "        data_dir: Path,\n",
    "        partition_dir: Path,\n",
    "        model_generator: Callable[[], Module],\n",
    "        single_client_device_capacity: dict[str, Any],\n",
    "        single_client_traces: dict[str, Any],\n",
    "        verbose: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialise the client.\n",
    "\n",
    "        A Client is given a unique id and the directory from which it can load its data.\n",
    "        Device capabilities and traces are also passed to the client.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            data_dir (Path): path to the dataset folder.\n",
    "            cid (int): Unique client id for a client used to map it to its data\n",
    "                partition\n",
    "            partition_dir (Path): The directory containing data for each client/client\n",
    "                id\n",
    "            model_generator (Callable[[], Module]): The model generator function\n",
    "            single_client_device_capacity (dict[str, Any]): dictionary containing info\n",
    "                about device capabilities.\n",
    "            single_client_traces (dict[str, Any]): dictionary describing client device\n",
    "                traces.\n",
    "            verbose (bool): boolean describing whether the client should print or not.\n",
    "        \"\"\"\n",
    "        super().__init__(cid, partition_dir, model_generator, data_dir)\n",
    "        self.device_capacity = single_client_device_capacity\n",
    "        self.trace = single_client_traces\n",
    "        self.verbose = verbose\n",
    "        self.properties: dict[str, Any] = {\n",
    "            \"tensor_type\": \"numpy.ndarray\",\n",
    "            \"cid\": self.cid,\n",
    "            \"device_capacity\": self.device_capacity,\n",
    "            \"traces\": self.trace,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self, parameters: NDArrays, config: dict[str, Scalar], **kwargs: dict[str, Any]\n",
    "    ) -> tuple[NDArrays, int, dict]:\n",
    "        \"\"\"Receive and train a model on the local client data.\n",
    "\n",
    "        Also, the function checks if the client is active at the current time step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            net (NDArrays): Pytorch model parameters\n",
    "            config (Dict[str, Scalar]): Dictionary describing the training parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            tuple[NDArrays, int, dict]: Returns the updated model, the size of the\n",
    "            raise ModelSizeNotFoundError()\n",
    "        \"\"\"\n",
    "        # We need to include model size to compute communications costs as part of our\n",
    "        # systems-aware simulation. Importantly, since Flower now accepts failures this\n",
    "        # will only cause the client to return a failure\n",
    "        if \"model_size\" not in config:\n",
    "            raise ModelSizeNotFoundError(\"Model size not found in config\")\n",
    "\n",
    "        # We need to compute the number of samples in the training set\n",
    "        # As such we set n_batches to the number of batches which the set contains\n",
    "        completion_time = get_client_completion_time(\n",
    "            single_client_device_capacity=self.device_capacity,\n",
    "            batch_size=int(config[\"batch_size\"]),\n",
    "            n_batches=int(\n",
    "                int(config[\"epochs\"])\n",
    "                * self.get_train_set_size()\n",
    "                / int(config[\"batch_size\"])\n",
    "            ),\n",
    "            model_size=float(config[\"model_size\"]),\n",
    "        )\n",
    "        # Add up the communication and computation times\n",
    "        total_time = completion_time[\"communication\"] + completion_time[\"computation\"]\n",
    "\n",
    "        # Store the result in the trace\n",
    "        self.trace[\"duration\"] = total_time\n",
    "\n",
    "        if self.verbose:\n",
    "            log(\n",
    "                INFO,\n",
    "                \"Client %s\\n--------\\n\\t\\t\"\n",
    "                \"Current virtual clock time: %s\\n\\t\\t\"\n",
    "                \"Duration: %s\\n\\t\\t\"\n",
    "                \"Traces: %s\\n\\t\\t\"\n",
    "                \"Predicted completion: %s\\n\\t\\t\"\n",
    "                \"Active: %s\\n--------\\n\",\n",
    "                self.cid,\n",
    "                config[\"current_virtual_clock\"],\n",
    "                total_time,\n",
    "                self.trace,\n",
    "                int(config[\"current_virtual_clock\"]) + int(total_time),\n",
    "                is_active(\n",
    "                    self.trace, int(config[\"current_virtual_clock\"]) + int(total_time)\n",
    "                ),\n",
    "            )\n",
    "        if \"current_virtual_clock\" not in config:\n",
    "            raise IntentionalDropoutError(\"Current virtual clock not found in config\")\n",
    "        if not is_active(\n",
    "            self.trace, int(config[\"current_virtual_clock\"]) + int(total_time)\n",
    "        ):\n",
    "            raise IntentionalDropoutError(\n",
    "                f\"Client {self.cid} is not active at the current time step\"\n",
    "            )\n",
    "\n",
    "        n_samples = len(self._create_data_loader(config, \"train\"))\n",
    "\n",
    "        # NOTE: We do not need to train for this experiments\n",
    "        # params, n_samples, results = super().fit(parameters, config)\n",
    "        results = {\n",
    "            \"client_completion_time\": total_time,\n",
    "            \"computation\": self.device_capacity[\"computation\"],\n",
    "            \"communication\": self.device_capacity[\"communication\"],\n",
    "            \"num_samples\": n_samples,\n",
    "            \"cid\": self.cid,\n",
    "        }\n",
    "\n",
    "        return parameters, n_samples, results\n",
    "\n",
    "    def evaluate(\n",
    "        self, parameters: NDArrays, config: dict[str, Scalar], **kwargs: dict[str, Any]\n",
    "    ) -> tuple[float, int, dict]:\n",
    "        \"\"\"Receive and test a model on the local client data.\n",
    "\n",
    "        Also, the function checks if the client is active at the current time step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            net (NDArrays): Pytorch model parameters\n",
    "            config (Dict[str, Scalar]): Dictionary describing the testing parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            tuple[float, int, dict]: Returns the loss accumulate during testing, the\n",
    "                size of the local dataset and other metrics such as accuracy\n",
    "        \"\"\"\n",
    "        if \"model_size\" not in config:\n",
    "            raise ModelSizeNotFoundError(\"Model size not found in config\")\n",
    "\n",
    "        # Estimate time based on number of batches in dataset\n",
    "        completion_time = get_client_completion_time(\n",
    "            single_client_device_capacity=self.device_capacity,\n",
    "            batch_size=int(config[\"batch_size\"]),\n",
    "            n_batches=int(self.get_test_set_size() / int(config[\"batch_size\"])),\n",
    "            model_size=float(config[\"model_size\"]),\n",
    "        )\n",
    "\n",
    "        # Compute total time\n",
    "        total_time = completion_time[\"communication\"] + completion_time[\"computation\"]\n",
    "\n",
    "        # Store result in trace\n",
    "        self.trace[\"duration\"] = total_time\n",
    "        if self.verbose:\n",
    "            log(\n",
    "                INFO,\n",
    "                \"Client %s\\n--------\\n\\t\\t\"\n",
    "                \"Current virtual clock time: %s\\n\\t\\t\"\n",
    "                \"Duration: %s\\n\\t\\t\"\n",
    "                \"Traces: %s\\n\\t\\t\"\n",
    "                \"Predicted completion: %s\\n\\t\\t\"\n",
    "                \"Active: %s\\n--------\\n\",\n",
    "                self.cid,\n",
    "                config[\"current_virtual_clock\"],\n",
    "                total_time,\n",
    "                self.trace,\n",
    "                int(config[\"current_virtual_clock\"]) + int(total_time),\n",
    "                is_active(\n",
    "                    self.trace, int(config[\"current_virtual_clock\"]) + int(total_time)\n",
    "                ),\n",
    "            )\n",
    "        if \"current_virtual_clock\" not in config:\n",
    "            raise IntentionalDropoutError(\"Current virtual clock not found in config\")\n",
    "        if not is_active(\n",
    "            self.trace, int(config[\"current_virtual_clock\"]) + int(total_time)\n",
    "        ):\n",
    "            raise IntentionalDropoutError(\n",
    "                f\"Client {self.cid} is not active at the current time step\"\n",
    "            )\n",
    "\n",
    "        n_samples = len(self._create_data_loader(config, \"test\"))\n",
    "\n",
    "        # NOTE: We do not need to train for this experiments\n",
    "        # params, n_samples, results = super().fit(parameters, config)\n",
    "        results = {\n",
    "            \"client_completion_time\": total_time,\n",
    "            \"computation\": self.device_capacity[\"computation\"],\n",
    "            \"communication\": self.device_capacity[\"communication\"],\n",
    "            \"num_samples\": n_samples,\n",
    "            \"cid\": self.cid,\n",
    "        }\n",
    "\n",
    "        return 0.0, n_samples, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppY8wuyGmSC0"
   },
   "outputs": [],
   "source": [
    "def get_flower_client_with_traces_generator(\n",
    "    clients_device_capacity: list[dict[str, Any]],\n",
    "    clients_traces: list[dict[str, Any]],\n",
    "    model_generator: Callable[[], Module],\n",
    "    data_dir: Path,\n",
    "    partition_dir: Path,\n",
    "    mapping_fn: Callable[[int], int] | None = None,\n",
    ") -> Callable[[str], FlowerClient]:\n",
    "    \"\"\"Wrap the client instance generator.\n",
    "\n",
    "    This provides the client generator with a model generator function.\n",
    "    Also, the partition directory must be passed.\n",
    "    A mapping function could be used for filtering/ordering clients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        clients_device_capacity (list[dict[str, Any]]): list containing the clients\n",
    "            device capabilities.\n",
    "        clients_traces (list[dict[str, Any]]): list containing the clients traces.\n",
    "        data_dir (Path): path to the datasßet folder.\n",
    "        model_generator (Callable[[], Module]): model generator function.\n",
    "        partition_dir (Path): directory containing the partition.\n",
    "        mapping_fn (Callable[[int], int] | None): function mapping sorted/filtered\n",
    "            ids to real cid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Callable[[str], FlowerClient]: client instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def client_fn(cid: str) -> FlowerClientTraces:\n",
    "        \"\"\"Create a single client instance given the client id `cid`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            cid (str): client id, Flower requires this to of type str.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            FlowerRayClientTraces: client instance.\n",
    "        \"\"\"\n",
    "        log(INFO, \"Getting client with id %s\", cid)\n",
    "        client = FlowerClientTraces(\n",
    "            # NOTE: passing the called `cid` here to allow for different mapping between\n",
    "            # data and devices\n",
    "            single_client_device_capacity=clients_device_capacity[int(cid)],\n",
    "            single_client_traces=clients_traces[int(cid)],\n",
    "            # NOTE: the mapping is only applied here, this is due to control the data\n",
    "            # mapping\n",
    "            cid=(mapping_fn(int(cid)) if mapping_fn is not None else int(cid)),\n",
    "            data_dir=data_dir,\n",
    "            partition_dir=partition_dir,\n",
    "            model_generator=model_generator,\n",
    "            # NOTE: you may want to comment out the following line or to set the\n",
    "            # verbosity to False\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        client.device = get_device()\n",
    "        return client\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 2025 Alexandru-Andrei Iacob, Lorenzo Sani"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "OUKMZp23mSCr",
    "yl5cv19OkMow"
   ],
   "provenance": [
    {
     "file_id": "1SZkrnGtgfuwvfRRP7UBAQ34BwUvmAMUh",
     "timestamp": 1705415599157
    },
    {
     "file_id": "1d-GiJwYlC-WdoGu_5vFlltF9tw3xC5NO",
     "timestamp": 1676304906256
    },
    {
     "file_id": "1R7bMyDO1RnPXzgWmi4DDWj0Mo-tHPZhw",
     "timestamp": 1676299669844
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "labs-MC7Y7X2c-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
